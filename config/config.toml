[ring0]
heartbeat_interval_sec = 2
heartbeat_timeout_sec = 6
max_cpu_percent = 80.0
max_memory_percent = 92.0
max_disk_percent = 90.0

[ring0.git]
ring2_path = "ring2"

[ring0.fitness]
db_path = "data/protea.db"

[ring0.evolution]
seed = 42
cooldown_sec = 3600
skill_max_count = 100
# Adaptive evolution: skip LLM calls when scores plateau to save tokens.
# plateau_window = how many recent scores to check for stagnation.
# plateau_epsilon = max score variance to consider "plateaued".
# When plateaued, evolution is skipped unless a user directive is pending.
plateau_window = 3
plateau_epsilon = 0.03

[ring1]
claude_model = "claude-sonnet-4-5-20250929"
claude_max_tokens = 16384
max_prompt_history = 5

[ring1.autonomy]
enabled = true
idle_threshold_sec = 600
check_interval_sec = 300

[ring1.tools]
workspace_path = "."
shell_timeout = 120
max_tool_rounds = 50

[ring1.task_executor]
prefer_local_skills = true   # match tasks to skills and recommend them to the LLM

[ring1.task_api]
enabled = true
host = "127.0.0.1"
port = 8877
secret_env = "PROTEA_TASK_API_SECRET"

[ring1.portal]
enabled = false
host = "127.0.0.1"
port = 8888

[ring1.dashboard]
enabled = true
host = "127.0.0.1"
port = 8899

[ring1.embeddings]
provider = "local"              # "openai" | "local" | "none"
api_key_env = "OPENAI_API_KEY"  # environment variable name
model = "text-embedding-3-small"
dimensions = 256

# [ring1.llm]
# provider = "openai"         # anthropic | openai | deepseek | qwen
# api_key_env = "OPENAI_API_KEY"
# model = "gpt-4o"
# max_tokens = 4096
# api_url = ""                # custom URL override (optional)

# --- Qwen 3.5 (千问 3.5) example ---
# [ring1.llm]
# provider = "qwen"
# api_key_env = "DASHSCOPE_API_KEY"
# model = "qwen3.5-plus"
# max_tokens = 8192

# --- MiniMax ---
# [ring1.llm]
# provider = "minimax"
# api_key_env = "MINIMAX_API_KEY"
# model = "MiniMax-Text-01"
# max_tokens = 8192

# --- Kimi (Moonshot) ---
# [ring1.llm]
# provider = "kimi"
# api_key_env = "MOONSHOT_API_KEY"
# model = "moonshot-v1-8k"
# max_tokens = 8192

# --- Gemini (Google) ---
# [ring1.llm]
# provider = "gemini"
# api_key_env = "GEMINI_API_KEY"
# model = "gemini-2.0-flash"
# max_tokens = 8192

# --- Ollama (local) ---
# [ring1.llm]
# provider = "ollama"
# model = "llama3"
# api_url = "http://localhost:11434/v1/chat/completions"
# max_tokens = 8192

[ring1.skill_sandbox]
enabled = true
base_dir = "data/skill_envs"
max_envs = 10
# Extra packages beyond the built-in allowlist (merged with validator defaults).
extra_allowed_packages = []

[ring1.task_sync]
enabled = true
interval_sec = 7200          # 2 hours between sync cycles
max_discover_per_sync = 5    # max templates to download per cycle

[ring1.convergence]
enabled = true
cluster_window_sec = 900
min_corrections = 2
similarity_threshold = 0.5
cooldown_sec = 600

[ring1.nudge]
enabled = true
interval_sec = 600          # 10 min between proactive nudges
max_daily_nudges = 20       # generous for testing phase

[ring1.proactive]
enabled = true
morning_hour = 9
evening_hour = 21
check_interval_sec = 1800
max_proactive_actions_per_day = 5

[ring1.user_profile]
implicit_extraction = true
extraction_rate_limit_sec = 300
moment_aggregation_threshold = 3
confidence_decay_rate = 0.98
max_preferences = 50

[ring1.telegram]
enabled = true

[ring1.matrix]
enabled = false
homeserver = ""     # e.g. "https://matrix.org"
room_id = ""        # e.g. "!abc:matrix.org"

[registry]
enabled = true
url = "https://protea-hub-production.up.railway.app"
node_id = "mac-mini"
host = "127.0.0.1"
port = 8761
db_path = "data/registry.db"
